## What is Latency?

Latency represents the time it takes data to flow from one point in a system to another point in the system. For example how long it takes for a client to send data to a server and receive a response from the server.

If a system has a high latency it means it will lag a lot. Applications like video streaming platforms, multiplayer video games etc need real time updates and so they require low latency.

Latency value is based on the amount of data being sent over the network and also the proximity of the components involved in the transfer.

For example, 
- reading a message from my RAM to my screen will take about 250 Micro Seconds
- reading the same information from my SSD and displaying it on my screen might take about 1000 micro seconds

This value is way higher when we talk about Network calls like API requests.
Making an API call from a local client to a local server both running on my PC will have a lower latency than when I make an API call from a client in California and a server in Netherlands.

## What Is Throughput?

Throughput is the amount of work a machine can do per unit time. For example, the amount of network request a server can receive or respond to per second is measured as request per second.

## What is The Relationship Between Latency and Throughput?

They can affect each other for example, a server that has 1GB/S throughput will most probably improve the latency of the system when compared with a server that has 500MB/S throughput.

As much as they are related, they do not always directly affect or correlate with each other. 

#ToBeContinued

